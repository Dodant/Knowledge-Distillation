# Knowledge-Distillation
## Is it possible to effectively train student models using logits with intervention? (Tentative Title) 
[Yechan Kim](https://github.com/unique-chan) and [Jungyun Oh](https://github.com/Dodant)

ðŸš§ Under Construction! (Do not fork this repository yet!)

### This repository contains:
- Python3 / Pytorch code for response-based knowledge distillation



### Prerequisites
- See `requirements.txt` for details.
~~~ME
torch
torchvision
tqdm            # not mandatory
tensorboard     # not mandatory
~~~


### Contribution
If you find any bugs or have opinions for further improvements, feel free to contact us (yechankim@gm.gist.ac.kr or maestr.oh@gm.gist.ac.kr). All contributions are welcome.


### Reference
1. https://github.com/weiaicunzai/pytorch-cifar100
2. https://github.com/peterliht/knowledge-distillation-pytorch
